\section*{Facilities, Equipment, and Other Resources}

\subsection*{The University of South Florida (USF)}

The proposed research will be performed at USF, a high-impact, global research university dedicated to student success. USF was established in 1956 as a public university and is a comprehensive multi-campus research university serving more than 49,000 students. The USF System is an evolving multi-campus system of higher education with fiscally autonomous, yet complementary, independently accredited institutions located in Tampa (including USF Health), St. Petersburg, and Sarasota-Manatee. The university employs more than 1,800 full-time instructional faculty and over 7,400 full-time staff. It has a \$1.7 billion annual budget, and an annual economic impact of \$4.4 billion (source: USF). 

\subsection*{Research Computing at USF}

For any large-scale computational need, the project will utilize the advanced computing resources administered by the Research Computing (RC) unit at USF. RC hosts a cluster computer, which consists of approximately 500 nodes with nearly 7200 cores running Red Hat Enterprise Linux 6. The cluster is built on the condominium model with 24TB of memory shared across the nodes in various configurations. Remote system and file access is available from anywhere via a web interface, including computational job submission and monitoring. RC provides and supports more than 120 scientific software packages including GAMS, MATLAB, R, SAS, SPSS, COIN-OR, GLPK, and Julia. RC staff members are available to facilitate use of the cluster, as well as provide direct assistance with research projects that require high-performance computing. User education and training sessions are also provided several times per semester. 

\subsection*{Department of Computer Science \& Engineering at USF}

The Department of Computer Science and Engineering at USF occupies a suite of offices and laboratories in a single building in the USF College of Engineering complex. Support from the full facilities, personnel, and equipment will be available for the duration of this project. The department operates several high performance computing platforms as well as numerous scientific workstations, Linux/Windows PCs, and dedicated Linux workstations. Recent upgrades include 2 16-core Linux servers with 128GB RAM each and 60TB of storage each to be used for the (/home) traditional storage for both faculty and students, and 5 8-core servers, with 32GB RAM and 1TB storage each, are used for file/print sharing, Active Directory, DHCP, DNS as well as web pages. 

\subsection*{Collaboration with Indiana University (IU)}

As part of this early-stage interdisciplinary collaboration, the PIs will engage with two additional collaborators during the course of this project --- Dr. Filippo Menczer and Dr. Alessandro Flammini from the School of Informatics, Computing, and Engineering (SICE) at Indiana University Bloomington. Drs. Flammini and Menczer are included in this project as `unfunded collaborators'; letters stating their intent to collaborate are attached as part of the Supplementary Documentation. Their contribution will be primarily focused on the development of algorithms for news recommendation to diverse audiences, robustness analysis, and simulation studies.

Collaboration with Drs. Flammini and Menczer will also let the PIs tap into resources available at Indiana University Bloomington. Before joining USF, PI Ciampaglia was a research scientist at Indiana University. He has worked in closed collaboration with Flammini and Menczer, and has retained access to IU's computer network and its computational resources. 

Indiana University is a national leader in the deployment and use of advanced information technology and cyberinfrastructure in support of research and education. 
% These centralized resources are available at no charge to graduate students, postdoctoral researchers, and faculty for their research needs.  
Some of the key facilities are described in the following.

\subsubsection*{Supercomputers and computer clusters} 

A number of systems provide researchers with access to some of the most powerful academic supercomputers in the US. These are: Big Red II, IU’s flagship supercomputer (1,020 CPUs, 676 GPUs, 1 petaflop); the Karst high-throughput computing cluster (546 CPUs, 85.1 teraflops), and the recently-inaugurated Carbonate high-RAM cluster (1,920 CPUs, 22 terabytes of RAM, 76.8 teraflops), which recently replaced the aging Mason system and provides and 18-fold increase over its predecessor.

\subsubsection*{Advanced I/O and backup systems} 

The Data Capacitor II is a 3.5-petabyte distributed storage that provides extremely fast input/output and massive short-term storage, enabling analyses of very large data sets and very large scale simulations.

The Scholarly Data Archive (SDA) is IU’s long-term backup and is capable of holding up to 15 petabytes of data. With mirrored tape silos on two separate campuses (in Bloomington and Indianapolis), this very secure (HIPAA-aligned) long-term storage system ensures that data are stored securely and reliably, even in the face of catastrophic technical failures or natural disasters.

\subsubsection*{Shared research servers} 

Thanks to previous funding from the NSF (Award No.~IIS-0811994), a more nimble computational infrastructure will be also available for the duration of this project.
These machines are part of the resources of the Center for Complex Networks and Systems Research (CNetS). The Center is a research unit within IU's SICE; Drs. Flammini and Menczer are some of its founding members and PI Ciampaglia spent time as postdoctoral fellow there prior to joining USF. 

The machines are managed directly by CNetS members, with help from professional system administrators of SICE. They include four servers, each with 16 cores and large amounts of memory ranging from 32GB to 128GB. In addition to local storage, these servers are also attached to a 20TB shared GFS storage area network (SAN), with fast network I/O. 
